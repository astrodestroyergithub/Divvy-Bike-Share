{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ce7100",
   "metadata": {},
   "source": [
    "# Novel Approach and Analysis Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0aee33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.8.6)\n",
      "Collecting pytorch_forecasting\n",
      "  Downloading pytorch_forecasting-0.10.3-py3-none-any.whl (141 kB)\n",
      "     ------------------------------------ 141.4/141.4 kB 270.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (2022.11.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (4.64.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (21.3)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (0.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (0.5.0)\n",
      "Requirement already satisfied: tensorboardX>=2.2 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch-lightning) (1.22.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch_forecasting) (3.4.3)\n",
      "Collecting scipy<2.0,>=1.8\n",
      "  Downloading scipy-1.10.1-cp310-cp310-win_amd64.whl (42.5 MB)\n",
      "     --------------------------------------- 42.5/42.5 MB 75.8 kB/s eta 0:00:00\n",
      "Collecting optuna<3.0.0,>=2.3.0\n",
      "  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n",
      "     ------------------------------------ 308.2/308.2 kB 130.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-learn<1.2,>=0.24 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch_forecasting) (1.0.2)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.3.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch_forecasting) (1.3.5)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pytorch_forecasting) (0.13.5)\n",
      "Requirement already satisfied: requests in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.9.4-py3-none-any.whl (210 kB)\n",
      "     ------------------------------------ 210.5/210.5 kB 188.4 kB/s eta 0:00:00\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Collecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-2.0.4-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 210.6 kB/s eta 0:00:00\n",
      "Collecting cliff\n",
      "  Downloading cliff-4.2.0-py3-none-any.whl (81 kB)\n",
      "     --------------------------------------- 81.0/81.0 kB 79.5 kB/s eta 0:00:00\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas<2.0.0,>=1.3.0->pytorch_forecasting) (2.8.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn<1.2,>=0.24->pytorch_forecasting) (1.1.0)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.57.0->pytorch-lightning) (0.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pytorch_forecasting) (8.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pytorch_forecasting) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->pytorch_forecasting) (0.11.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels->pytorch_forecasting) (0.5.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (21.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/optuna/\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: six in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from patsy>=0.5.2->statsmodels->pytorch_forecasting) (1.16.0)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-2.0.2-cp310-cp310-win_amd64.whl (192 kB)\n",
      "     ------------------------------------- 192.2/192.2 kB 79.8 kB/s eta 0:00:00\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     --------------------------------------- 78.7/78.7 kB 95.3 kB/s eta 0:00:00\n",
      "Collecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.4.3-py3-none-any.whl (147 kB)\n",
      "     ------------------------------------ 147.2/147.2 kB 121.8 kB/s eta 0:00:00\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-5.0.0-py3-none-any.whl (49 kB)\n",
      "     -------------------------------------- 49.6/49.6 kB 114.8 kB/s eta 0:00:00\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
      "Collecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-3.6.0-py3-none-any.whl (27 kB)\n",
      "Collecting autopage>=0.4.0\n",
      "  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.9.24)\n",
      "Collecting pyreadline3\n",
      "  Downloading pyreadline3-3.4.1-py3-none-any.whl (95 kB)\n",
      "     -------------------------------------- 95.2/95.2 kB 118.2 kB/s eta 0:00:00\n",
      "Collecting pyperclip>=1.6\n",
      "  Using cached pyperclip-1.8.2.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cmd2>=1.0.0->cliff->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (0.2.5)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n",
      "     ------------------------------------ 112.7/112.7 kB 198.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\tamojit\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from Mako->alembic->optuna<3.0.0,>=2.3.0->pytorch_forecasting) (2.0.1)\n",
      "Building wheels for collected packages: pyperclip\n",
      "  Building wheel for pyperclip (setup.py): started\n",
      "  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=a66f086de2867dcacaed6dc58724f0ce066c02d21c9d4947bd4ab54676cc57b2\n",
      "  Stored in directory: c:\\users\\tamojit\\appdata\\local\\pip\\cache\\wheels\\3c\\77\\81\\aaa2802e9b0553585f2789c6f2756b50a09a01d2848423bb15\n",
      "Successfully built pyperclip\n",
      "Installing collected packages: pyreadline3, pyperclip, zipp, scipy, PrettyTable, pbr, Mako, greenlet, colorlog, cmd2, cmaes, autopage, stevedore, sqlalchemy, importlib-metadata, cliff, alembic, optuna, pytorch_forecasting\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.3\n",
      "    Uninstalling scipy-1.7.3:\n",
      "      Successfully uninstalled scipy-1.7.3\n",
      "Successfully installed Mako-1.2.4 PrettyTable-3.6.0 alembic-1.9.4 autopage-0.5.1 cliff-4.2.0 cmaes-0.9.1 cmd2-2.4.3 colorlog-6.7.0 greenlet-2.0.2 importlib-metadata-6.0.0 optuna-2.10.1 pbr-5.11.1 pyperclip-1.8.2 pyreadline3-3.4.1 pytorch_forecasting-0.10.3 scipy-1.10.1 sqlalchemy-2.0.4 stevedore-5.0.0 zipp-3.15.0\n"
     ]
    }
   ],
   "source": [
    "pip install torch pytorch-lightning pytorch_forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a13c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_forecasting import TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002a4b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>stop_time</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>from_station_id</th>\n",
       "      <th>from_station_name</th>\n",
       "      <th>to_station_id</th>\n",
       "      <th>to_station_name</th>\n",
       "      <th>user_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>...</th>\n",
       "      <th>from_longitude</th>\n",
       "      <th>from_location</th>\n",
       "      <th>to_latitude</th>\n",
       "      <th>to_longitude</th>\n",
       "      <th>to_location</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-06-30 10:48:00</th>\n",
       "      <td>10009</td>\n",
       "      <td>06/30/2013 10:58:00 AM</td>\n",
       "      <td>354</td>\n",
       "      <td>557</td>\n",
       "      <td>58</td>\n",
       "      <td>Ashland Ave &amp; Armitage Ave</td>\n",
       "      <td>60</td>\n",
       "      <td>Dayton St &amp; North Ave</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Female</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.668879</td>\n",
       "      <td>POINT (-87.668879 41.916017)</td>\n",
       "      <td>41.910578</td>\n",
       "      <td>-87.649422</td>\n",
       "      <td>POINT (-87.649422 41.910578)</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-30 10:51:00</th>\n",
       "      <td>10017</td>\n",
       "      <td>06/30/2013 02:19:00 PM</td>\n",
       "      <td>559</td>\n",
       "      <td>12496</td>\n",
       "      <td>37</td>\n",
       "      <td>Dearborn St &amp; Adams St</td>\n",
       "      <td>34</td>\n",
       "      <td>Cannon Dr &amp; Fullerton Ave</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.629791</td>\n",
       "      <td>POINT (-87.629791 41.879356)</td>\n",
       "      <td>41.926756</td>\n",
       "      <td>-87.634429</td>\n",
       "      <td>POINT (-87.634429 41.926756)</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-30 11:27:00</th>\n",
       "      <td>10106</td>\n",
       "      <td>06/30/2013 11:39:00 AM</td>\n",
       "      <td>616</td>\n",
       "      <td>699</td>\n",
       "      <td>17</td>\n",
       "      <td>Wood St &amp; Division St</td>\n",
       "      <td>69</td>\n",
       "      <td>Damen Ave &amp; Pierce Ave</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.672730</td>\n",
       "      <td>POINT (-87.67273 41.90332)</td>\n",
       "      <td>41.909396</td>\n",
       "      <td>-87.677692</td>\n",
       "      <td>POINT (-87.677692 41.909396)</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-30 11:35:00</th>\n",
       "      <td>10133</td>\n",
       "      <td>06/30/2013 11:53:00 AM</td>\n",
       "      <td>136</td>\n",
       "      <td>1093</td>\n",
       "      <td>127</td>\n",
       "      <td>Lincoln Ave &amp; Fullerton Ave</td>\n",
       "      <td>26</td>\n",
       "      <td>McClurg Ct &amp; Illinois St</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.649260</td>\n",
       "      <td>POINT (-87.64926 41.925905)</td>\n",
       "      <td>41.891020</td>\n",
       "      <td>-87.617300</td>\n",
       "      <td>POINT (-87.6173 41.89102)</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-30 11:56:00</th>\n",
       "      <td>10189</td>\n",
       "      <td>06/30/2013 01:02:00 PM</td>\n",
       "      <td>279</td>\n",
       "      <td>3957</td>\n",
       "      <td>72</td>\n",
       "      <td>State St &amp; 16th St</td>\n",
       "      <td>15</td>\n",
       "      <td>Racine Ave &amp; 18th St</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-87.625813</td>\n",
       "      <td>POINT (-87.625813 41.860384)</td>\n",
       "      <td>41.858166</td>\n",
       "      <td>-87.656495</td>\n",
       "      <td>POINT (-87.656495 41.858166)</td>\n",
       "      <td>2013-06-30</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     trip_id               stop_time  bike_id  trip_duration  \\\n",
       "start_time                                                                     \n",
       "2013-06-30 10:48:00    10009  06/30/2013 10:58:00 AM      354            557   \n",
       "2013-06-30 10:51:00    10017  06/30/2013 02:19:00 PM      559          12496   \n",
       "2013-06-30 11:27:00    10106  06/30/2013 11:39:00 AM      616            699   \n",
       "2013-06-30 11:35:00    10133  06/30/2013 11:53:00 AM      136           1093   \n",
       "2013-06-30 11:56:00    10189  06/30/2013 01:02:00 PM      279           3957   \n",
       "\n",
       "                     from_station_id            from_station_name  \\\n",
       "start_time                                                          \n",
       "2013-06-30 10:48:00               58   Ashland Ave & Armitage Ave   \n",
       "2013-06-30 10:51:00               37       Dearborn St & Adams St   \n",
       "2013-06-30 11:27:00               17        Wood St & Division St   \n",
       "2013-06-30 11:35:00              127  Lincoln Ave & Fullerton Ave   \n",
       "2013-06-30 11:56:00               72           State St & 16th St   \n",
       "\n",
       "                     to_station_id            to_station_name   user_type  \\\n",
       "start_time                                                                  \n",
       "2013-06-30 10:48:00             60      Dayton St & North Ave  Subscriber   \n",
       "2013-06-30 10:51:00             34  Cannon Dr & Fullerton Ave    Customer   \n",
       "2013-06-30 11:27:00             69     Damen Ave & Pierce Ave    Customer   \n",
       "2013-06-30 11:35:00             26   McClurg Ct & Illinois St    Customer   \n",
       "2013-06-30 11:56:00             15       Racine Ave & 18th St    Customer   \n",
       "\n",
       "                     gender  ...  from_longitude  \\\n",
       "start_time                   ...                   \n",
       "2013-06-30 10:48:00  Female  ...      -87.668879   \n",
       "2013-06-30 10:51:00     NaN  ...      -87.629791   \n",
       "2013-06-30 11:27:00     NaN  ...      -87.672730   \n",
       "2013-06-30 11:35:00     NaN  ...      -87.649260   \n",
       "2013-06-30 11:56:00     NaN  ...      -87.625813   \n",
       "\n",
       "                                    from_location  to_latitude to_longitude  \\\n",
       "start_time                                                                    \n",
       "2013-06-30 10:48:00  POINT (-87.668879 41.916017)    41.910578   -87.649422   \n",
       "2013-06-30 10:51:00  POINT (-87.629791 41.879356)    41.926756   -87.634429   \n",
       "2013-06-30 11:27:00    POINT (-87.67273 41.90332)    41.909396   -87.677692   \n",
       "2013-06-30 11:35:00   POINT (-87.64926 41.925905)    41.891020   -87.617300   \n",
       "2013-06-30 11:56:00  POINT (-87.625813 41.860384)    41.858166   -87.656495   \n",
       "\n",
       "                                      to_location        date  year month  \\\n",
       "start_time                                                                  \n",
       "2013-06-30 10:48:00  POINT (-87.649422 41.910578)  2013-06-30  2013     6   \n",
       "2013-06-30 10:51:00  POINT (-87.634429 41.926756)  2013-06-30  2013     6   \n",
       "2013-06-30 11:27:00  POINT (-87.677692 41.909396)  2013-06-30  2013     6   \n",
       "2013-06-30 11:35:00     POINT (-87.6173 41.89102)  2013-06-30  2013     6   \n",
       "2013-06-30 11:56:00  POINT (-87.656495 41.858166)  2013-06-30  2013     6   \n",
       "\n",
       "                     day  day_of_week  \n",
       "start_time                             \n",
       "2013-06-30 10:48:00   30       Sunday  \n",
       "2013-06-30 10:51:00   30       Sunday  \n",
       "2013-06-30 11:27:00   30       Sunday  \n",
       "2013-06-30 11:35:00   30       Sunday  \n",
       "2013-06-30 11:56:00   30       Sunday  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('divvy-tripdata_cleaned.csv', index_col=1)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.sort_index(inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d06f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.resample('1h').mean().replace(0., np.nan)\n",
    "earliest_time = data.index.min()\n",
    "df=data[['trip_id','bike_id','trip_duration','from_station_id','to_station_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "372a6fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for label in df:\n",
    "\n",
    "    ts = df[label]\n",
    "\n",
    "    start_date = min(ts.fillna(method='ffill').dropna().index)\n",
    "    end_date = max(ts.fillna(method='bfill').dropna().index)\n",
    "\n",
    "    active_range = (ts.index >= start_date) & (ts.index <= end_date)\n",
    "    ts = ts[active_range].fillna(0.)\n",
    "\n",
    "    tmp = pd.DataFrame({'trip_duration': ts})\n",
    "    date = tmp.index\n",
    "\n",
    "    tmp['hours_from_start'] = (date - earliest_time).seconds / 60 / 60 + (date - earliest_time).days * 24\n",
    "    tmp['hours_from_start'] = tmp['hours_from_start'].astype('int')\n",
    "  \n",
    "    tmp['days_from_start'] = (date - earliest_time).days\n",
    "    tmp['date'] = date\n",
    "    tmp['bikeshare_id'] = label\n",
    "    tmp['hour'] = date.hour\n",
    "    tmp['day'] = date.day\n",
    "    tmp['day_of_week'] = date.dayofweek\n",
    "    tmp['month'] = date.month\n",
    "\n",
    "    # stack all time series vertically\n",
    "    df_list.append(tmp)\n",
    "\n",
    "time_df = pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "# match results in the original paper\n",
    "time_df = time_df[(time_df['days_from_start'] >= 1096)\n",
    "                & (time_df['days_from_start'] < 1346)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3848aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df[['bikeshare_id','trip_duration']].groupby('bikeshare_id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "# batch size=64\n",
    "# number heads=4, hidden sizes=160, lr=0.001, gr_clip=0.1\n",
    "\n",
    "max_prediction_length = 24\n",
    "max_encoder_length = 7*24\n",
    "training_cutoff = time_df[\"hours_from_start\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    time_df[lambda x: x.hours_from_start <= training_cutoff],\n",
    "    time_idx=\"hours_from_start\",\n",
    "    target=\"trip_duration\",\n",
    "    group_ids=[\"bikeshare_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2, \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"bikeshare_id\"],\n",
    "    time_varying_known_reals=[\"hours_from_start\",\"day\",\"day_of_week\", \"month\", 'hour'],\n",
    "    time_varying_unknown_reals=['trip_duration'],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"bikeshare_id\"], transformation=\"softplus\"\n",
    "    ),  # we normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, time_df, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for  our model\n",
    "batch_size = 64 \n",
    "# if you have a strong GPU, feel free to increase the number of workers  \n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=5, verbose=True, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  \n",
    "logger = TensorBoardLogger(\"lightning_logs\")  \n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=45,\n",
    "    accelerator='gpu', \n",
    "    devices=1,\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger)\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.001,\n",
    "    hidden_size=160,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=160,\n",
    "    output_size=7,  # there are 7 quantiles by default: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10, \n",
    "    reduce_on_plateau_patience=4)\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(best_model_path)\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our saved model again\n",
    "!unzip model.zip\n",
    "best_model_path='lightning_logs/lightning_logs/version_1/checkpoints/epoch=8-step=4212.ckpt'\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef535c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard - logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06684467",
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "\n",
    "# average p50 loss overall\n",
    "print((actuals - predictions).abs().mean().item())\n",
    "# average p50 loss per time series\n",
    "print((actuals - predictions).abs().mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at what the raw_predictions variable contains\n",
    "\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "print(raw_predictions._fields)\n",
    "# ('prediction', \n",
    "# 'encoder_attention', \n",
    "# 'decoder_attention', \n",
    "# 'static_variables', \n",
    "# 'encoder_variables', \n",
    "# 'decoder_variables', \n",
    "# 'decoder_lengths', \n",
    "# 'encoder_lengths')\n",
    "\n",
    "print('\\n')\n",
    "print(raw_predictions['prediction'].shape)\n",
    "# torch.Size([5, 24, 7])\n",
    "\n",
    "# we get predictions of 5 time-series for 24 days.\n",
    "# for each day we get 7 predictions - these are the 7 quantiles:\n",
    "# [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "# we are mostly interested in the 4th quantile which represents, let's say, the 'median loss'\n",
    "# fyi, although docs use the term quantiles, the most accurate term are percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):  # plot all 5 consumers\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True,ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "raw_prediction, x = best_tft.predict(\n",
    "    training.filter(lambda x: (x.consumer_id == \"MT_004\") & (x.time_idx_first_prediction == 26512)),\n",
    "    mode=\"raw\",\n",
    "    return_x=True,\n",
    ")\n",
    "best_tft.plot_prediction(x, raw_prediction, idx=0, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce82444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder data is the last lookback window: we get the last 1 week (168 datapoints) for all 5 consumers = 840 total datapoints\n",
    "\n",
    "encoder_data = time_df[lambda x: x.hours_from_start > x.hours_from_start.max() - max_encoder_length]\n",
    "last_data = time_df[lambda x: x.hours_from_start == x.hours_from_start.max()]\n",
    "\n",
    "# decoder_data is the new dataframe for which we will create predictions. \n",
    "# decoder_data df should be max_prediction_length*consumers = 24*5=120 datapoints long : 24 datapoints for each cosnumer\n",
    "# we create it by repeating the last hourly observation of every consumer 24 times since we do not really have new test data\n",
    "# and later we fix the columns\n",
    "\n",
    "decoder_data = pd.concat(\n",
    "    [last_data.assign(date=lambda x: x.date + pd.offsets.Hour(i)) for i in range(1, max_prediction_length + 1)],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "# fix the new columns\n",
    "decoder_data[\"hours_from_start\"] = (decoder_data[\"date\"] - earliest_time).dt.seconds / 60 / 60 + (decoder_data[\"date\"] - earliest_time).dt.days * 24\n",
    "decoder_data['hours_from_start'] = decoder_data['hours_from_start'].astype('int')\n",
    "decoder_data[\"hours_from_start\"] += encoder_data[\"hours_from_start\"].max() + 1 - decoder_data[\"hours_from_start\"].min()\n",
    "\n",
    "decoder_data[\"month\"] = decoder_data[\"date\"].dt.month.astype(np.int64)\n",
    "decoder_data[\"hour\"] = decoder_data[\"date\"].dt.hour.astype(np.int64)\n",
    "decoder_data[\"day\"] = decoder_data[\"date\"].dt.day.astype(np.int64)\n",
    "decoder_data[\"day_of_week\"] = decoder_data[\"date\"].dt.dayofweek.astype(np.int64)\n",
    "\n",
    "new_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# create out-of-sample predictions for MT_002\n",
    "new_prediction_data=new_prediction_data.query(\" consumer_id == 'MT_002'\")\n",
    "new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n",
    "best_tft.plot_prediction(new_x, new_raw_predictions, idx=0, show_future_observed=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3020aab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)\n",
    "interpretation = best_tft.interpret_output(raw_predictions, reduction=\"sum\")\n",
    "best_tft.plot_interpretation(interpretation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14a2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis on the training set\n",
    "\n",
    "predictions, x = best_tft.predict(train_dataloader, return_x=True)\n",
    "predictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\n",
    "best_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new study\n",
    "study = optimize_hyperparameters(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    model_path=\"optuna_test\",\n",
    "    n_trials=1,\n",
    "    max_epochs=1,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(30, 128),\n",
    "    hidden_continuous_size_range=(30, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.001, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False \n",
    ")\n",
    "\n",
    "# save study results\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# print best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976f462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
